{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e6a142-64c7-4dc7-8cc7-bd5e2e564155",
   "metadata": {},
   "source": [
    "# **NEURAL NETWORKS FROM SCRATCH**\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d71c9cd-591a-450e-afc3-e7c3684dabfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 20:23:53.295711: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-18 20:23:53.295739: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, r2_score\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd6e7b-c838-428a-87eb-37fe70cb879d",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    "## **Define the class NeuralNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c473e8-4aaa-4bf3-8d39-a941dc470a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet :\n",
    "    \"\"\"\n",
    "    The class builds a Neural Network for binary classification and regression\n",
    "    It is possible to define:\n",
    "    - The task of the problem (\"classification\", \"regression\")\n",
    "    - The activation function (\"sigmoid\",\"tanh\",\"relu\",\"leaky_relu\",\"elu\",\"swish\")\n",
    "    - The hidden layers (a tuple where each element represents the number of nodes in each layer)\n",
    "    - The Gradient Descent algorithm (\"batch\",\"adam\")\n",
    "    - The learning rate\n",
    "    - The type of regularization (\"ridge\",\"lasso\")\n",
    "    - The regularization factor lambda\n",
    "    - The maximum number of iterations\n",
    "    - The momentum factor in the weights optimization\n",
    "    - A random state for reproducible results\n",
    "    - A flag to display results while processing\n",
    "    - A flag to apply early stopping to the algorithm\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    Modules:\n",
    "    - Training: training the Neural Network\n",
    "    - Predict: class prediction (for classification) and value (for regression)\n",
    "    - Predict_proba: probability prediction (only for classification)\n",
    "    - Score: performance metric (different metrics for classification and regression)\n",
    "    \n",
    "    Attributes:\n",
    "    - last_iter: last iteration of training\n",
    "    - best_weights: best weights found with training\n",
    "    - cost_function_tr: cost function values for the training set up to the last iteration\n",
    "    - cost_function_te: cost function values for the validation set up to the last iteration\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task, function = \"sigmoid\", Hidden_layers = (5,), algo = \"batch\",\n",
    "                 alpha = 0.3, regularization = \"ridge\", Lambda = 0.0, Max_iter = 1000,\n",
    "                 momentum = 0.8, random_state = None, verbose = 0,\n",
    "                 early_stopping = True) :\n",
    "        self.task = task\n",
    "        self.function = function\n",
    "        self.Hidden_layers = Hidden_layers\n",
    "        self.algo = algo\n",
    "        self.alpha = alpha\n",
    "        self.regularization = regularization\n",
    "        self.Lambda = Lambda\n",
    "        self.Max_iter = Max_iter\n",
    "        self.momentum = momentum\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.early_stopping = early_stopping\n",
    "    \n",
    "    def act(function,X) :\n",
    "        \"\"\"\n",
    "        The function returns the transformed values according to the specified activation function\n",
    "        - function    activation function \n",
    "        - X           input matrix (data with no target column)\n",
    "        \"\"\"\n",
    "        \n",
    "        if function == \"sigmoid\" :\n",
    "            return 1. / (1. + np.exp(-X))\n",
    "        elif function == \"tanh\" :\n",
    "            return np.tanh(X)\n",
    "        elif function == \"relu\" :\n",
    "            return np.where(X>=0,X,0)\n",
    "        elif function == \"leaky_relu\" :\n",
    "            return np.where(X>=0,X,0.1*X)\n",
    "        elif function == \"elu\" :\n",
    "            return np.where(X>=0,X, 0.1*(np.exp(X) - 1))           #alpha = 0.1\n",
    "        elif function == \"swish\" :\n",
    "            return X * (1./(1.+np.exp(-X)))\n",
    "        else :\n",
    "            raise ValueError(\"The activation function is not valid\")\n",
    "\n",
    "    \n",
    "    def derivative(function,X) :\n",
    "        \"\"\"\n",
    "        The function returns the transformed values according to the derivative of the specified activation function\n",
    "        - function    activation function \n",
    "        - X           input matrix (data with no target column)\n",
    "        \"\"\"\n",
    "        \n",
    "        if function == \"sigmoid\" :\n",
    "            return (NeuralNet.act(function,X)) * (1 - NeuralNet.act(function,X))\n",
    "        elif function == \"tanh\" :\n",
    "            return (1 - NeuralNet.act(function,X)**2)\n",
    "        elif function == \"relu\" :\n",
    "            return np.where(X>=0,1,0)\n",
    "        elif function == \"leaky_relu\" :\n",
    "            return np.where(X>=0,1,0.1)\n",
    "        elif function == \"elu\" :\n",
    "            return np.where(X>=0,1, 0.1*np.exp(X)) \n",
    "        elif function == \"swish\" :\n",
    "            return NeuralNet.act(\"sigmoid\",X) * (1 + X - NeuralNet.act(function,X))\n",
    "        else :\n",
    "            raise ValueError(\"The activation function is not valid\")\n",
    "            \n",
    "    \n",
    "    def randw(self, Lin,Lout, function, X) :\n",
    "        \"\"\"\n",
    "        Source: https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        The function returns a matrix of random weights with dimensions Lout x (Lin 1)\n",
    "        - Lin         dimension of the input layer\n",
    "        - Lout        dimension of the output layer\n",
    "        - function    activation function\n",
    "        - X           input matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.random_state is not None :\n",
    "            np.random.seed(self.random_state)\n",
    "        if (function == \"relu\") or (function == \"leaky_relu\") or (function == \"elu\") :\n",
    "            return np.random.normal(0.0, np.sqrt(2/X.shape[0]),(Lin+1)*Lout).reshape(Lout,Lin+1)\n",
    "        elif (function == \"sigmoid\") or (function == \"tanh\") or (function == \"swish\") :\n",
    "            epsilon = (6 / (Lin+Lout))**0.5;\n",
    "            return np.random.rand(Lout,Lin+1)*2*epsilon - epsilon\n",
    "        else :\n",
    "            raise ValueError(\"The activation function is not valid\")\n",
    "        \n",
    "        \n",
    "    def Bias(m,X) :\n",
    "        \"\"\"\n",
    "        The function returns the matrix with a bias column made of 1s\n",
    "         - m           number of elements to add (on one row)\n",
    "         - X           input matrix (data with no target column)\n",
    "        \"\"\"\n",
    "        if m != X.shape[1] :\n",
    "            raise ValueError(\"The value of m must match the number of columns of X\")\n",
    "        else :    \n",
    "            return np.vstack([np.ones((1,m)),X]) \n",
    "\n",
    "    \n",
    "    def Forward_propagation(self, X, T) :        \n",
    "        \"\"\"\n",
    "        The function returns the values obtained in the the last layer through the forward propagation algorithm\n",
    "        - X           input matrix (data with no target column)\n",
    "        - T           tuple containing n matrices (i.e. weights)\n",
    "        \"\"\"\n",
    "        \n",
    "        m = X.shape[0]                                 #Number of observations\n",
    "        n = len(T)                                     #Number of matrices (i.e. weights)\n",
    "        X = np.hstack([np.ones((m,1)),X])              #Add bias column\n",
    "        a = NeuralNet.act(self.function,np.dot(T[0],X.T))            #a1 = X.T\n",
    "\n",
    "        for i in np.arange(3,n+2) :\n",
    "            if i==(n+1) :                              #Final layer\n",
    "                if self.task == \"classification\" :\n",
    "                    a = NeuralNet.act('sigmoid',np.dot(T[i-2],NeuralNet.Bias(m,a)))\n",
    "                else :\n",
    "                    a = np.dot(T[i-2],NeuralNet.Bias(m,a))\n",
    "                break\n",
    "            else :                                     #Other layers\n",
    "                a = NeuralNet.act(self.function,np.dot(T[i-2],NeuralNet.Bias(m,a)))\n",
    "\n",
    "        return a.T\n",
    "    \n",
    "    \n",
    "    def ForwardBack_propagation(self, X, y, T) :\n",
    "        \"\"\"\n",
    "        The function returns the values obtained in each layer (through forward propagation) and the errors (through back propagaton)\n",
    "        - X           input matrix (data with no target column)\n",
    "        - yy          target variable transformed with OneHotEncoder (number of columns = number of classes)\n",
    "        - T           tuple containing n matrices (i.e. weights)\n",
    "        \"\"\"\n",
    "        \n",
    "        m = X.shape[0]                                   #Number of observations\n",
    "        n = len(T)                                       #Number of matrices (i.e. weights)\n",
    "        X = np.hstack([np.ones((m,1)),X])                #Add bias column\n",
    "        a2 = NeuralNet.act(self.function,np.dot(T[0],X.T))    #a1 = X.T\n",
    "        ANODES = (a2,)\n",
    "        DNODES = ()\n",
    "\n",
    "        #Modify target column for regression\n",
    "        if self.task == \"regression\" :\n",
    "            y = np.array(y).reshape(len(y),1)\n",
    "\n",
    "        #Forward propagation loop\n",
    "        for i in np.arange(3,n+3) :\n",
    "            if i==(n+1) :                              #Final layer\n",
    "                if self.task == \"classification\" :\n",
    "                    a = NeuralNet.act('sigmoid',np.dot(T[i-2],NeuralNet.Bias(m,ANODES[i-3])))\n",
    "                else :\n",
    "                    a = np.dot(T[i-2],NeuralNet.Bias(m,ANODES[i-3]))\n",
    "                ANODES = ANODES + (a,)\n",
    "                DNODES = (a - y.T,) + DNODES\n",
    "                break\n",
    "            else :                                     #Other layers\n",
    "                ANODES = ANODES + (NeuralNet.act(self.function,np.dot(T[i-2],NeuralNet.Bias(m,ANODES[i-3]))),)\n",
    "\n",
    "        #Back propagation loop\n",
    "        for i in np.arange(2,n+1)[::-1] :\n",
    "            DNODES = (( np.dot(T[i-1].T,DNODES[0]) * NeuralNet.derivative(self.function,NeuralNet.Bias(m,ANODES[i-2])) )[1:,:],) + DNODES\n",
    "\n",
    "        return ANODES, DNODES\n",
    "    \n",
    "    \n",
    "    def J_Grad(self, X, y, labels, T) :\n",
    "        \"\"\"\n",
    "        The function returns the Cost function (with the regularization term) and the Gradient\n",
    "        - X           input matrix (data with no target column)\n",
    "        - y           target variable\n",
    "        - labels      number of differen classes\n",
    "        - Lambda      regularization factor\n",
    "        - T           tuple containing n matrices (i.e. weights)\n",
    "        \"\"\"\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        n = len(T)\n",
    "\n",
    "        #Create target array (OneHotEncoder - number of columns = number of classes)\n",
    "        if self.task == \"classification\" :\n",
    "            yy = np.zeros((len(y),labels))\n",
    "            for i in np.arange(0,m) :\n",
    "                for k in np.arange(0,labels) :\n",
    "                    if y[i] == k :\n",
    "                        yy[i,k] = 1\n",
    "        else :\n",
    "            yy = y\n",
    "\n",
    "        #Forward and Back propagation\n",
    "        anodes, dnodes = NeuralNet.ForwardBack_propagation(self, X, yy, T)\n",
    "\n",
    "        #Gradient of the Cost function\n",
    "        Delta1 = np.dot(dnodes[0],np.hstack([np.ones((m,1)),X]))\n",
    "        Delta2 = np.dot(dnodes[1],NeuralNet.Bias(m,anodes[0]).T)\n",
    "        if self.regularization == \"ridge\" :\n",
    "            t1grad = (Delta1+(np.hstack([np.zeros((T[0].shape[0],1)), T[0][:,1:]]))*self.Lambda)/m\n",
    "            t2grad = (Delta2+(np.hstack([np.zeros((T[1].shape[0],1)), T[1][:,1:]]))*self.Lambda)/m  \n",
    "        else :\n",
    "            t1grad = (Delta1+(np.hstack([np.zeros((T[0].shape[0],1)), np.sign(T[0][:,1:])]))*self.Lambda)/m\n",
    "            t2grad = (Delta2+(np.hstack([np.zeros((T[1].shape[0],1)), np.sign(T[1][:,1:])]))*self.Lambda)/m  \n",
    "            \n",
    "        TGRAD = (t1grad,) + (t2grad,)\n",
    "        for i in np.arange(3,n+1) :\n",
    "            Delta = np.dot(dnodes[i-1],NeuralNet.Bias(m,anodes[i-2]).T)\n",
    "            if self.regularization == \"ridge\" :\n",
    "                tgrad = (Delta+(np.hstack([ np.zeros((T[i-1].shape[0],1)), T[i-1][:,1:]]))*self.Lambda)/m \n",
    "            else :\n",
    "                tgrad = (Delta+(np.hstack([ np.zeros((T[i-1].shape[0],1)), T[i-1][:,1:]]))*self.Lambda)/m \n",
    "            TGRAD = TGRAD + (tgrad,)\n",
    "\n",
    "        #Regularization term and Cost function\n",
    "        if self.regularization == \"ridge\" :\n",
    "            REG = sum([np.sum(i[:,1:]**2) for i in T])*self.Lambda/(2*m)  \n",
    "        else :\n",
    "            REG = sum([np.sum(abs(i[:,1:])) for i in T])*self.Lambda/(2*m)  \n",
    "            \n",
    "        #Cost function\n",
    "        if self.task == \"classification\" :\n",
    "            J = sum(sum(-np.log10(anodes[-1])*yy.T - np.log10(1-anodes[-1])*(1-yy).T))/m + REG\n",
    "        else :\n",
    "            J = (dnodes[-1]**2).sum()/(2*m) + REG\n",
    "\n",
    "        return J, TGRAD\n",
    "\n",
    "    \n",
    "    def Prediction(self, X, T) :\n",
    "        \"\"\"\n",
    "        The function returns the corresponding predicted class using fixed weights.\n",
    "        This is specific of a binary classification problem. In the case of regression, it return only the predicted values\n",
    "        - X           input matrix (data with no target column)\n",
    "        - T           tuple containing n matrices (i.e. weights)\n",
    "        \"\"\"\n",
    "\n",
    "        Final = NeuralNet.Forward_propagation(self, X, T)\n",
    "        if self.task == \"classification\" :\n",
    "            Predictions = []\n",
    "            for i in np.arange(0,len(Final)) :\n",
    "                Predictions.append(np.argmax(Final[i,:]))\n",
    "            return Predictions\n",
    "        else :\n",
    "            return Final\n",
    "    \n",
    "    \n",
    "    def Score(self, X, y, metric) :\n",
    "        \"\"\"\n",
    "        The function returns the accuracy (for classification) and RMSE for regression\n",
    "        - X           input matrix (data with no target column)\n",
    "        - y           target variable\n",
    "        - metric      metric to evaluate the performance\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.task == \"classification\" :\n",
    "            if metric == \"accuracy\" :\n",
    "                return (self.Predict(X) == y).sum()/len(y)\n",
    "            elif metric == \"precision\" :\n",
    "                return precision_score(y, self.Predict(X))\n",
    "            elif metric == \"recall\" :\n",
    "                return recall_score(y, self.Predict(X))\n",
    "            elif metric == \"f1score\" :\n",
    "                return f1_score(y, self.Predict(X))\n",
    "            elif metric == \"auc\" :\n",
    "                return roc_auc_score(y, self.Predict_proba(X))\n",
    "            else :\n",
    "                raise ValueError(\"Misspelled or inappropriate metric for %s\" % self.task)\n",
    "            \n",
    "        else :\n",
    "            if metric == \"rmse\" :\n",
    "                return np.sqrt(((self.Predict(X) - np.array(y).reshape(len(y),1))**2).sum()/len(y))\n",
    "            elif metric == \"mae\" :\n",
    "                return (abs(self.Predict(X) - np.array(y).reshape(len(y),1))).sum()/len(y)\n",
    "            elif metric == \"mpe\" :\n",
    "                return 100*( abs((self.Predict(X) - np.array(y).reshape(len(y),1))/np.array(y).reshape(len(y),1)) ).sum()/len(y)\n",
    "            elif metric == \"r2\" :\n",
    "                return r2_score(y, self.Predict(X) )\n",
    "            else :\n",
    "                raise ValueError(\"Misspelled or inappropriate metric for %s\" % self.task)\n",
    "                \n",
    "                \n",
    "    def Training(self, X_train, y_train, X_test, y_test) :\n",
    "        \"\"\"\n",
    "        The function returns the last iteration, a tuple containing the weights, the cost function value at each iteration for the training and validation sets\n",
    "        - X_train               input matrix (data with no target column)\n",
    "        - y_train               target variable\n",
    "        - X_test          matrix for cross-validation\n",
    "        - y_test          target variable for cross-validation\n",
    "        \"\"\"\n",
    "        \n",
    "        if (len(X_train) != len(y_train)) or (len(X_test) != len(y_test)):\n",
    "            raise ValueError(\"Data and target have different dimensions\")\n",
    "         \n",
    "        else :\n",
    "            inputs = X_train.shape[1]               #Number of features\n",
    "            Cost_value = 1e09                       #Value to start early stopping for classification\n",
    "            tolerance = 1e-07                       #Tolerance value for regression early stopping\n",
    "            self.flag = \"Convergent cost function\"  #Initialization of the \"convergence flag\" (if cost function is divergent, it will be changed)\n",
    "            \n",
    "            #Number of labels (in the output)\n",
    "            if self.task == \"classification\" :\n",
    "                labels = len(np.unique(y_train))\n",
    "            else :\n",
    "                labels = 1\n",
    "\n",
    "            #Random initialization of weights\n",
    "            n = len(self.Hidden_layers)\n",
    "            THETA = (NeuralNet.randw(self, inputs, self.Hidden_layers[0], self.function, X_train),)\n",
    "            for i in np.arange(0,n) :\n",
    "                if i==n-1 :\n",
    "                    THETA = THETA + (NeuralNet.randw(self, self.Hidden_layers[i], labels, self.function, X_train),)\n",
    "                    break\n",
    "                else :\n",
    "                    THETA = THETA + (NeuralNet.randw(self, self.Hidden_layers[i], self.Hidden_layers[i+1], self.function, X_train),)\n",
    "            \n",
    "            #Initialization to zero of the change in the weights (to compute the momentum)\n",
    "            Change = tuple([0.0*x for x in THETA])\n",
    "            #Initialization of the momenta for the Adam algorithm\n",
    "            M_beta = tuple([0.0*x for x in THETA])\n",
    "            V_beta = tuple([0.0*x for x in THETA])\n",
    "            #Default hyperparameters of the Adam algorithm\n",
    "            beta1 = 0.9\n",
    "            beta2 = 0.999\n",
    "            epsilon = 1e-08\n",
    "            \n",
    "            #Iterative training (i: epoch)\n",
    "            Cost_tr, Cost_te = [], []\n",
    "            for i in range(self.Max_iter) :\n",
    "                #Cost function and gradient\n",
    "                J_tr, G_tr = NeuralNet.J_Grad(self, X_train, y_train, labels, THETA)\n",
    "                J_te, G_te = NeuralNet.J_Grad(self, X_test, y_test, labels, THETA)\n",
    "                #Update cost function lists\n",
    "                Cost_tr.append(J_tr)\n",
    "                Cost_te.append(J_te)\n",
    "\n",
    "                #Show results\n",
    "                if self.verbose != 0 :\n",
    "                    if self.task == \"classification\" :\n",
    "                        Accuracy_tr = (NeuralNet.Prediction(self, X_train, THETA) == y_train).sum()/len(y_train)\n",
    "                        Accuracy_te = (NeuralNet.Prediction(self, X_test, THETA) == y_test).sum()/len(y_test)\n",
    "                        print('\\rIteration: {}/{} ----- Training cost: {:.5f} - Validation cost: {:.5f} --- Training accuracy: {:.5f} - Validation accuracy: {:.5f}'.format(i,\n",
    "                                                                                                                                              self.Max_iter,\n",
    "                                                                                                                                              J_tr,J_te,\n",
    "                                                                                                                                              Accuracy_tr,Accuracy_te), end='')\n",
    "                    else :\n",
    "                        RMSE_tr = np.sqrt(((NeuralNet.Prediction(self, X_train, THETA) - np.array(y_train).reshape(len(y_train),1))**2).sum()/len(y_train))\n",
    "                        RMSE_te = np.sqrt(((NeuralNet.Prediction(self, X_test, THETA) - np.array(y_test).reshape(len(y_test),1))**2).sum()/len(y_test))\n",
    "                        print('\\rIteration: {}/{} ----- Training cost: {:.5f} - Validation cost: {:.5f} --- Training RMSE: {:.5f} - Validation RMSE: {:.5f}'.format(i,\n",
    "                                                                                                                                              self.Max_iter,\n",
    "                                                                                                                                              J_tr,J_te,\n",
    "                                                                                                                                              RMSE_tr,RMSE_te), end='')\n",
    "                #Early stopping\n",
    "                #The condition i>Iter is added to avoid initial flactuations\n",
    "                Iter = 50\n",
    "                broken = 0\n",
    "                if self.task == \"classification\" :\n",
    "                    if (i>Iter) & (self.early_stopping == True) :        \n",
    "                        diff = 100\n",
    "                        if (Cost_te[Iter-1] < Cost_tr[Iter-1]) and ((J_tr - J_te) < 0) :\n",
    "                            broken = 1\n",
    "                        else :\n",
    "                            if J_te < Cost_value :\n",
    "                                Cost_value = J_te\n",
    "                            else :\n",
    "                                broken = 1\n",
    "                else :\n",
    "                    if (i>Iter) & (self.early_stopping == True) :        \n",
    "                        if abs(J_tr - Cost_value) > tolerance :\n",
    "                            Cost_value = J_tr\n",
    "                        else :\n",
    "                            broken = 1\n",
    "\n",
    "                #Stop when the cost function is divergent\n",
    "                broken2 = 0\n",
    "                if (i>Iter) and (Cost_tr[-1]>Cost_tr[-10]) and (Cost_tr[-1]>Cost_tr[-50]) :\n",
    "                    broken, broken2 = 1, 1\n",
    "                    \n",
    "                if broken==1 :\n",
    "                    if broken2==1 :\n",
    "                        self.flag = \"Divergent cost function\"\n",
    "                    else :\n",
    "                        self.flag = \"Convergent cost function\"\n",
    "                        self.last_iter = i\n",
    "                        self.best_weights = THETA\n",
    "                        self.cost_function_tr = Cost_tr\n",
    "                        self.cost_function_te = Cost_te\n",
    "                    break\n",
    "                \n",
    "                #Update weights\n",
    "                if self.algo == \"batch\" :\n",
    "                    #Compute changes in the weights (with momentum)\n",
    "                    NewChange = ()\n",
    "                    for k in range(n+1) :\n",
    "                        NewChange = (NewChange + (self.alpha*G_tr[k] + self.momentum*Change[k],))\n",
    "\n",
    "                    #Update weights\n",
    "                    for k in range(n+1) :\n",
    "                        THETA = (THETA + (THETA[0] - NewChange[k],))[1:]\n",
    "\n",
    "                    #Update change\n",
    "                    Change = NewChange\n",
    "                elif self.algo == \"adam\":\n",
    "                    #Adam algorithm without momentum\n",
    "                    if self.momentum != 0.0 :\n",
    "                        raise ValueError(\"Set the momentum to zero for the Adam algorithm\")\n",
    "                    #Compute first and second momenta\n",
    "                    NewM_beta = ()\n",
    "                    NewV_beta = ()\n",
    "                    for k in range(n+1) :\n",
    "                        NewM_beta = (NewM_beta + (beta1*M_beta[k] + (1-beta1)*G_tr[k],))\n",
    "                        NewV_beta = (NewV_beta + (beta2*V_beta[k] + (1-beta2)*(G_tr[k]**2),))\n",
    "                    \n",
    "                    #Bias correction of the momenta (with static decay of the beta parameters)\n",
    "                    Mhat = tuple([x/(1 - beta1**(i+1)) for x in NewM_beta])\n",
    "                    Vhat = tuple([x/(1 - beta2**(i+1)) for x in NewV_beta])\n",
    "                    \n",
    "                    #Update weights\n",
    "                    for k in range(n+1) :\n",
    "                        THETA = (THETA + (THETA[0] - self.alpha*Mhat[k]/(np.sqrt(Vhat[k]) + epsilon),))[1:]\n",
    "                    \n",
    "                    #Update momenta\n",
    "                    M_beta = NewM_beta\n",
    "                    V_beta = NewV_beta\n",
    "                else :\n",
    "                    raise ValueError(\"The algorithm used for optimizing the weights is not valid\")\n",
    "                    \n",
    "            self.last_iter = i\n",
    "            self.best_weights = THETA\n",
    "            self.cost_function_tr = Cost_tr\n",
    "            self.cost_function_te = Cost_te\n",
    "        \n",
    "    \n",
    "    def Predict(self, X) :\n",
    "        \"\"\"\n",
    "        The function returns the corresponding predicted class using the best weights found with training. \n",
    "        This is specific of a binary classification problem. In the case of regression, it return only the predicted values\n",
    "        - X           input matrix (data with no target column)\n",
    "        - T           tuple containing n matrices (i.e. weights)\n",
    "        \"\"\"\n",
    "\n",
    "        Final = NeuralNet.Forward_propagation(self, X, self.best_weights)\n",
    "        if self.task == \"classification\" :\n",
    "            Predictions = []\n",
    "            for i in np.arange(0,len(Final)) :\n",
    "                Predictions.append(np.argmax(Final[i,:]))\n",
    "            return Predictions\n",
    "        else :\n",
    "            return Final\n",
    "    \n",
    "    \n",
    "    def Predict_proba(self, X) :\n",
    "        \"\"\"\n",
    "        The function returns the final probabilities of getting the positive class and the corresponding predicted class\n",
    "        This is specific of a binary classification problem. \n",
    "        - X           input matrix (data with no target column)\n",
    "        - T           tuple containing n matrices (i.e. weights)\n",
    "        \"\"\"\n",
    "        \n",
    "        Final = NeuralNet.Forward_propagation(self, X, self.best_weights)\n",
    "        if self.task == \"classification\" :\n",
    "            Predictions = []\n",
    "            for i in np.arange(0,len(Final)) :\n",
    "                Predictions.append(np.argmax(Final[i,:]))\n",
    "            return Final[:,1]\n",
    "        else :\n",
    "            raise ValueError(\"No probabilities for %s\" % self.task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c634242-e60d-47c7-9a3c-6ee26313d614",
   "metadata": {
    "tags": []
   },
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "## **Datasets for classification and regression**\n",
    "\n",
    "The dataset for classification has been retrieved from https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names : it is about the *Pima Indians Diabetes*.\n",
    "\n",
    "The dataset for regression is taken from the repository https://github.com/SamuComqi92/Car_price_prediction (\"Car price\" - the training and test sets has been already corrected and scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d983db-013f-4d53-90f3-c608aab8bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data\n",
    "#CLASSIFICATION\n",
    "Data_class = pd.read_csv(\"dati2\")\n",
    "\n",
    "#Create a balanced dataset (268 records for each class)\n",
    "Data_class_bal = pd.concat([Data_class[Data_class.Class==0].sample(268, random_state=0),Data_class[Data_class.Class==1]])\n",
    "\n",
    "#Creation of X and y\n",
    "X_class = Data_class_bal.drop([\"Class\"],axis=1)\n",
    "y_class = Data_class_bal.Class\n",
    "X_class = np.array(X_class)\n",
    "y_class = np.array(y_class)\n",
    "\n",
    "#Creation of training and test sets\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class,y_class,test_size=0.3,random_state=0)\n",
    "\n",
    "#Feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_class)\n",
    "X_train_class = scaler.transform(X_train_class)\n",
    "X_test_class = scaler.transform(X_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d750974a-2e6d-4608-a4f6-3ca66046361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 18) (11, 18)\n"
     ]
    }
   ],
   "source": [
    "#REGRESSION\n",
    "Data_train_regr = pd.read_csv(\"Car_Dataset_train.csv\")\n",
    "Data_test_regr = pd.read_csv(\"Car_Dataset_test.csv\")\n",
    "Data_train_regr = Data_train_regr.iloc[:,1:]\n",
    "Data_test_regr = Data_test_regr.iloc[:,1:]\n",
    "\n",
    "#Shape of the datasets\n",
    "print(Data_train_regr.shape, Data_test_regr.shape)\n",
    "\n",
    "#Creation of X and y (for training and testing)\n",
    "X_train_regr = Data_train_regr.drop(\"CarPrice\",axis=1)\n",
    "y_train_regr = Data_train_regr.CarPrice\n",
    "X_test_regr = Data_test_regr.drop(\"CarPrice\",axis=1)\n",
    "y_test_regr = Data_test_regr.CarPrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbec1e1-1019-481a-b69e-2c831c850273",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "## **Training: classification & regression (example)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48c4aec-c615-4694-bf34-29cd36dc6029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch GD algorithm\n",
      "Iteration: 999/1000 ----- Training cost: 0.49454 - Validation cost: 0.53867 --- Training accuracy: 0.74333 - Validation accuracy: 0.66667\n",
      "Batch + Momentum GD algorithm\n",
      "Iteration: 748/1000 ----- Training cost: 0.44556 - Validation cost: 0.48758 --- Training accuracy: 0.76000 - Validation accuracy: 0.72000\n",
      "Adam GD algorithm\n",
      "Iteration: 76/1000 ----- Training cost: 0.43617 - Validation cost: 0.46066 --- Training accuracy: 0.75000 - Validation accuracy: 0.72000"
     ]
    }
   ],
   "source": [
    "#Classification\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_train_class,y_train_class,test_size = 0.2,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "#Batch algorithm\n",
    "print(\"Batch GD algorithm\")\n",
    "Model = NeuralNet(task=\"classification\", function = \"sigmoid\", Hidden_layers = (6,), \n",
    "              algo = \"batch\", alpha = 0.2, regularization = \"ridge\", Lambda = 0.0,\n",
    "              Max_iter = 1000, momentum = 0.0, early_stopping = True, \n",
    "              random_state = 42, verbose = 1)\n",
    "\n",
    "Model.Training(X_train_t, y_train_t, X_test_t, y_test_t)\n",
    "\n",
    "#Batch + Momentum algorithm\n",
    "print(\"\\nBatch + Momentum GD algorithm\")\n",
    "Model = NeuralNet(task=\"classification\", function = \"sigmoid\", Hidden_layers = (6,), \n",
    "              algo = \"batch\", alpha = 0.2, regularization = \"ridge\", Lambda = 0.0,\n",
    "              Max_iter = 1000, momentum = 0.85, early_stopping = True, \n",
    "              random_state = 42, verbose = 1)\n",
    "\n",
    "Model.Training(X_train_t, y_train_t, X_test_t, y_test_t)\n",
    "\n",
    "#Adam algorithm\n",
    "print(\"\\nAdam GD algorithm\")\n",
    "Model = NeuralNet(task=\"classification\", function = \"sigmoid\", Hidden_layers = (6,), \n",
    "              algo = \"adam\", alpha = 0.2, regularization = \"ridge\", Lambda = 0.0,\n",
    "              Max_iter = 1000, momentum = 0.0, early_stopping = True, \n",
    "              random_state = 42, verbose = 1)\n",
    "\n",
    "Model.Training(X_train_t, y_train_t, X_test_t, y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf306733-9dd7-4ea8-8f20-4b6111383b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch GD algorithm\n",
      "Iteration: 999/1000 ----- Training cost: 0.00389 - Validation cost: 0.00433 --- Training RMSE: 0.08820 - Validation RMSE: 0.09303\n",
      "Batch + Momentum GD algorithm\n",
      "Iteration: 999/1000 ----- Training cost: 0.00305 - Validation cost: 0.00330 --- Training RMSE: 0.07812 - Validation RMSE: 0.08122\n",
      "Adam GD algorithm\n",
      "Iteration: 452/1000 ----- Training cost: 0.00273 - Validation cost: 0.00286 --- Training RMSE: 0.07391 - Validation RMSE: 0.07560"
     ]
    }
   ],
   "source": [
    "#Regression\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_train_regr,y_train_regr,test_size = 0.2,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "#Batch algorithm\n",
    "print(\"Batch GD algorithm\")\n",
    "Model = NeuralNet(task=\"regression\", function = \"sigmoid\", Hidden_layers = (7,3), \n",
    "              algo = \"batch\", alpha = 0.1, regularization = \"ridge\", Lambda = 0.0,\n",
    "              Max_iter = 1000, momentum = 0.0, early_stopping = True, \n",
    "              random_state = 42, verbose = 1)\n",
    "\n",
    "Model.Training(X_train_t, y_train_t, X_test_t, y_test_t)\n",
    "\n",
    "#Batch + Momentum algorithm\n",
    "print(\"\\nBatch + Momentum GD algorithm\")\n",
    "Model = NeuralNet(task=\"regression\", function = \"sigmoid\", Hidden_layers = (7,3), \n",
    "              algo = \"batch\", alpha = 0.1, regularization = \"ridge\", Lambda = 0.0,\n",
    "              Max_iter = 1000, momentum = 0.85, early_stopping = True, \n",
    "              random_state = 42, verbose = 1)\n",
    "\n",
    "Model.Training(X_train_t, y_train_t, X_test_t, y_test_t)\n",
    "\n",
    "#Adam algorithm\n",
    "print(\"\\nAdam GD algorithm\")\n",
    "Model = NeuralNet(task=\"regression\", function = \"relu\", Hidden_layers = (7,3), \n",
    "              algo = \"adam\", alpha = 0.1, regularization = \"ridge\", Lambda = 0.0,\n",
    "              Max_iter = 1000, momentum = 0.0, early_stopping = True, \n",
    "              random_state = 42, verbose = 1)\n",
    "\n",
    "Model.Training(X_train_t, y_train_t, X_test_t, y_test_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
